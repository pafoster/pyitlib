<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>pyitlib &#8212; pyitlib 0.3.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/classic.css?v=2bf1fcf8" />
    
    <script src="_static/documentation_options.js?v=4621528c"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">pyitlib 0.3.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">pyitlib</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="toctree-wrapper compound">
</div>
<section id="pyitlib">
<h1>pyitlib<a class="headerlink" href="#pyitlib" title="Link to this heading">¶</a></h1>
<p>pyitlib is an MIT-licensed library of information-theoretic methods for data analysis and machine learning, implemented in Python and NumPy.</p>
<p>API documentation is available online at <a class="reference external" href="https://pafoster.github.io/pyitlib/">https://pafoster.github.io/pyitlib/</a>.</p>
<p>pyitlib implements the following 19 measures on discrete random variables:</p>
<ul class="simple">
<li><p>Entropy</p></li>
<li><p>Joint entropy</p></li>
<li><p>Conditional entropy</p></li>
<li><p>Cross entropy</p></li>
<li><p>Kullback-Leibler divergence</p></li>
<li><p>Symmetrised Kullback-Leibler divergence</p></li>
<li><p>Jensen-Shannon divergence</p></li>
<li><p>Mutual information</p></li>
<li><p>Normalised mutual information (7 variants)</p></li>
<li><p>Variation of information</p></li>
<li><p>Lautum information</p></li>
<li><p>Conditional mutual information</p></li>
<li><p>Co-information</p></li>
<li><p>Interaction information</p></li>
<li><p>Multi-information</p></li>
<li><p>Binding information</p></li>
<li><p>Residual entropy</p></li>
<li><p>Exogenous local information</p></li>
<li><p>Enigmatic information</p></li>
</ul>
<p>The following estimators are available for each of the measures:</p>
<ul class="simple">
<li><p>Maximum likelihood</p></li>
<li><p>Maximum a posteriori</p></li>
<li><p>James-Stein</p></li>
<li><p>Good-Turing</p></li>
</ul>
<p>Missing data are supported, either using placeholder values or NumPy masked arrays.</p>
<section id="installation-and-codebase">
<h2>Installation and codebase<a class="headerlink" href="#installation-and-codebase" title="Link to this heading">¶</a></h2>
<p>pyitlib is listed on the Python Package Index at <a class="reference external" href="https://pypi.python.org/pypi/pyitlib/">https://pypi.python.org/pypi/pyitlib/</a> and may be installed using <code class="docutils literal notranslate"><span class="pre">pip</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">pyitlib</span>
</pre></div>
</div>
<p>The codebase for pyitlib is available at <a class="reference external" href="https://github.com/pafoster/pyitlib">https://github.com/pafoster/pyitlib</a>.</p>
</section>
<section id="notes-for-getting-started">
<h2>Notes for getting started<a class="headerlink" href="#notes-for-getting-started" title="Link to this heading">¶</a></h2>
<p>Import the module <code class="docutils literal notranslate"><span class="pre">discrete_random_variable</span></code>, as well as NumPy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pyitlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">discrete_random_variable</span> <span class="k">as</span> <span class="n">drv</span>
</pre></div>
</div>
<p>The respective methods implemented in <code class="docutils literal notranslate"><span class="pre">discrete_random_variable</span></code> accept NumPy arrays as input. Let’s compute the entropy for an array containing discrete random variable realisations, based on maximum likelihood estimation and quantifying entropy in bits:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array(1.0)</span>
</pre></div>
</div>
<p>NumPy arrays are created automatically for any input which isn’t of the required type, by passing the input to np.array(). Let’s compute entropy, again based on maximum likelihood estimation, but this time using list input and quantifying entropy in nats:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">base</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="go">array(0.6931471805599453)</span>
</pre></div>
</div>
<p>Those methods with the suffix <code class="docutils literal notranslate"><span class="pre">_pmf</span></code> operate on arrays specifying probability mass assignments. For example, the analogous method call for computing the entropy of the preceding random variable realisations (with estimated equi-probable outcomes) is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy_pmf</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">base</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="go">0.69314718055994529</span>
</pre></div>
</div>
<p>It’s possible to specify missing data using placeholder values (the default placeholder value is <code class="docutils literal notranslate"><span class="pre">-1</span></code>). Elements equal to the placeholder value are subsequently ignored:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="go">array(1.0)</span>
</pre></div>
</div>
<p>In measures expressible in terms of joint entropy (such as conditional entropy, mutual information etc.), equally many realisations of respective random variables are required (with realisations coupled using a common index). Any missing data for random variable <code class="docutils literal notranslate"><span class="pre">X</span></code> results in the corresponding realisations for random variable <code class="docutils literal notranslate"><span class="pre">Y</span></code> being ignored, and vice versa. Thus, the following method calls yield equivalent results (note use of alternative placeholder value <code class="docutils literal notranslate"><span class="pre">None</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy_conditional</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="go">array(0.5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy_conditional</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="kc">None</span><span class="p">],</span> <span class="n">fill_value</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array(0.5)</span>
</pre></div>
</div>
<p>It’s alternatively possible to specify missing data using NumPy masked arrays:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">mask</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="go">array(1.0)</span>
</pre></div>
</div>
<p>In combination with any estimator other than maximum likelihood, it may be useful to specify alphabets containing unobserved outcomes. For example, we might seek to estimate the entropy in bits for the sequence of realisations <code class="docutils literal notranslate"><span class="pre">[1,1,1,1]</span></code>. Using maximum a posteriori estimation combined with the Perks prior (i.e. pseudo-counts of 1/L for each of L possible outcomes) and based on an alphabet specifying L=100 possible outcomes, we may use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;PERKS&#39;</span><span class="p">,</span> <span class="n">Alphabet_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="go">array(2.030522626645241)</span>
</pre></div>
</div>
<p>Multi-dimensional array input is supported based on the convention that <em>leading dimensions index random variables, with the trailing dimension indexing random variable realisations</em>. Thus, the following array specifies realisations for 3 random variables:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(3, 4)</span>
</pre></div>
</div>
<p>When using multi-dimensional arrays, any alphabets must be specified separately for each random variable represented in the multi-dimensional array, using placeholder values (or NumPy masked arrays) to pad out any unequally sized alphabets:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;PERKS&#39;</span><span class="p">,</span> <span class="n">Alphabet_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">),(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span> <span class="c1"># 3 alphabets required</span>
<span class="go">array([ 2.03052263,  2.81433872,  2.81433872])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span> <span class="c1"># padding required</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;PERKS&#39;</span><span class="p">,</span> <span class="n">Alphabet_X</span> <span class="o">=</span> <span class="n">A</span><span class="p">)</span>
<span class="go">array([ 0.46899559,  1.        ,  1.28669267])</span>
</pre></div>
</div>
<p>For ease of use, those methods operating on two random variable array arguments (such as <code class="docutils literal notranslate"><span class="pre">entropy_conditional</span></code>, <code class="docutils literal notranslate"><span class="pre">information_mutual</span></code> etc.) may be invoked with a single multi-dimensional array. In this way, we may compute mutual information for all pairs of random variables represented in the array as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">information_mutual</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 0.,  1.,  1.],</span>
<span class="go">       [ 0.,  1.,  1.]])</span>
</pre></div>
</div>
<p>The above is equivalent to setting the <code class="docutils literal notranslate"><span class="pre">cartesian_product</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">True</span></code> and specifying two random variable array arguments explicitly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">information_mutual</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cartesian_product</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 0.,  1.,  1.],</span>
<span class="go">       [ 0.,  1.,  1.]])</span>
</pre></div>
</div>
<p>By default, those methods operating on several random variable array arguments don’t determine all combinations of random variables exhaustively. Instead a one-to-one mapping is performed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">information_mutual</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># Mutual information between 3 pairs of random variables</span>
<span class="go">array([ 0.,  1.,  1.])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># Mutual information equivalent to entropy in above case</span>
<span class="go">array([ 0.,  1.,  1.])</span>
</pre></div>
</div>
<p>pyitlib provides basic support for pandas DataFrames/Series. Both these types are converted to NumPy masked arrays internally, while masking those data recorded as missing (based on .isnull()). Note that due to indexing random variable realisations using the trailing dimension of multi-dimensional arrays, we typically need to transpose DataFrames when estimating information-theoretic quantities:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/veekun/pokedex/master/pokedex/data/csv/pokemon.csv&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;base_experience&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span> <span class="c1"># Bin the data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">drv</span><span class="o">.</span><span class="n">information_mutual_normalised</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># Transposition required for comparing columns</span>
<span class="go">array([[ 1.        ,  0.32472696,  0.17745753],</span>
<span class="go">       [ 0.32729034,  1.        ,  0.13343504],</span>
<span class="go">       [ 0.17848175,  0.13315407,  1.        ]])</span>
</pre></div>
</div>
</section>
<section id="module-discrete_random_variable">
<span id="discrete-random-variable"></span><h2>discrete_random_variable<a class="headerlink" href="#module-discrete_random_variable" title="Link to this heading">¶</a></h2>
<p>This module implements various information-theoretic quantities for discrete
random variables.</p>
<p>For ease of reference, function names follow the following convention:</p>
<p>Function names beginning with “entropy” : Entropy measures</p>
<p>Function names beginning with “information” : Mutual information measures</p>
<p>Function names beginning with “divergence” : Divergence measures</p>
<p>Function names ending with “pmf” : Functions operating on arrays of probability
mass assignments (as opposed realisations of random variables)</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Generalises</p></th>
<th class="head"><p>Non-negativity</p></th>
<th class="head"><p>Symmetry</p></th>
<th class="head"><p>Identity</p></th>
<th class="head"><p>Metric properties</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.divergence_jensenshannon" title="discrete_random_variable.divergence_jensenshannon"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divergence_jensenshannon()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Square root is a metric</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.divergence_jensenshannon_pmf" title="discrete_random_variable.divergence_jensenshannon_pmf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divergence_jensenshannon_pmf()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Square root is a metric</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler" title="discrete_random_variable.divergence_kullbackleibler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divergence_kullbackleibler()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler_pmf" title="discrete_random_variable.divergence_kullbackleibler_pmf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divergence_kullbackleibler_pmf()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler_symmetrised" title="discrete_random_variable.divergence_kullbackleibler_symmetrised"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divergence_kullbackleibler_symmetrised()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler_symmetrised_pmf" title="discrete_random_variable.divergence_kullbackleibler_symmetrised_pmf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divergence_kullbackleibler_symmetrised_pmf()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.entropy" title="discrete_random_variable.entropy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.entropy_conditional" title="discrete_random_variable.entropy_conditional"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy_conditional()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.entropy_cross" title="discrete_random_variable.entropy_cross"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy_cross()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.entropy_cross_pmf" title="discrete_random_variable.entropy_cross_pmf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy_cross_pmf()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.entropy_joint" title="discrete_random_variable.entropy_joint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy_joint()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.entropy_pmf" title="discrete_random_variable.entropy_pmf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy_pmf()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.entropy_residual" title="discrete_random_variable.entropy_residual"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy_residual()</span></code></a></p></td>
<td><p>information_variation</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.information_binding" title="discrete_random_variable.information_binding"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_binding()</span></code></a></p></td>
<td><p>information_mutual</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.information_co" title="discrete_random_variable.information_co"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_co()</span></code></a></p></td>
<td><p>information_mutual</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.information_enigmatic" title="discrete_random_variable.information_enigmatic"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_enigmatic()</span></code></a></p></td>
<td></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.information_exogenous_local" title="discrete_random_variable.information_exogenous_local"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_exogenous_local()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.information_interaction" title="discrete_random_variable.information_interaction"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_interaction()</span></code></a></p></td>
<td><p>information_mutual</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.information_lautum" title="discrete_random_variable.information_lautum"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_lautum()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.information_multi" title="discrete_random_variable.information_multi"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_multi()</span></code></a></p></td>
<td><p>information_mutual</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.information_mutual" title="discrete_random_variable.information_mutual"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_mutual()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.information_mutual_conditional" title="discrete_random_variable.information_mutual_conditional"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_mutual_conditional()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#discrete_random_variable.information_mutual_normalised" title="discrete_random_variable.information_mutual_normalised"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_mutual_normalised()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>See docs</p></td>
<td><p>No</p></td>
<td><p>See docs</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#discrete_random_variable.information_variation" title="discrete_random_variable.information_variation"><code class="xref py py-meth docutils literal notranslate"><span class="pre">information_variation()</span></code></a></p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Is a metric</p></td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="abpl12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>AbPl12<span class="fn-bracket">]</span></span>
<p>Abdallah, S.A.; Plumbley, M.D.: A measure of statistical complexity based on predictive information with application to finite spin systems. In: Physics Letters A, Vol. 376, No. 4, 2012, P. 275-281.</p>
</div>
<div class="citation" id="bell03" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bell03<span class="fn-bracket">]</span></span>
<p>Bell, A.J.: The co-information lattice. In: Proceedings of the International Workshop on Independent Component Analysis and Blind Signal Separation. 2003.</p>
</div>
<div class="citation" id="coth06" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CoTh06<span class="fn-bracket">]</span></span>
<p>Cover, T.M.; Thomas, J.A.: Elements of information theory (2nd ed.). John Wiley &amp; Sons, 2006.</p>
</div>
<div class="citation" id="croo15" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Croo15<span class="fn-bracket">]</span></span>
<p>Crooks, G.E.: On measures of entropy and information. <a class="reference external" href="http://threeplusone.com/info">http://threeplusone.com/info</a>, retrieved 2017-03-16.</p>
</div>
<div class="citation" id="gasa95" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GaSa95<span class="fn-bracket">]</span></span>
<p>Gale, W.A.; Sampson, G.: Good‐Turing frequency estimation without tears. In: Journal of Quantitative Linguistics, Vol. 2, No. 3, 1995, P. 217-237.</p>
</div>
<div class="citation" id="han78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Han78<span class="fn-bracket">]</span></span>
<p>Han, T.S.: Nonnegative entropy measures of multivariate symmetric correlations. In: Information and Control, Vol. 36, 1978, P. 133-156.</p>
</div>
<div class="citation" id="hast09" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HaSt09<span class="fn-bracket">]</span></span>
<p>Hausser, J.; Strimmer, K.: Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks. In: Journal of Machine Learning Research, Vol. 10, 2009, P. 1469-1484.</p>
</div>
<div class="citation" id="jabr03" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>JaBr03<span class="fn-bracket">]</span></span>
<p>Jakulin, A.; Bratko, I.: Quantifying and visualizing attribute interactions. arXiv preprint cs/0308002, 2003.</p>
</div>
<div class="citation" id="jaec11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>JaEC11<span class="fn-bracket">]</span></span>
<p>James, R.G.; Ellison, C.J.; Crutchfield, J.P.: Anatomy of a bit: Information in a time series observation. In: Chaos: An Interdisciplinary Journal of Nonlinear Science, Vol. 21, No. 3, 2011.</p>
</div>
<div class="citation" id="lin91" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lin91<span class="fn-bracket">]</span></span>
<p>Lin, J.: Divergence measures based on the Shannon entropy. In: IEEE Transactions on Information theory, Vol. 37, No. 1, 1991, P. 145-151.</p>
</div>
<div class="citation" id="meil03" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Meil03<span class="fn-bracket">]</span></span>
<p>Meilă, M.: Comparing clusterings by the variation of information. In: Learning theory and kernel machines. Springer, 2003, P. 173-187.</p>
</div>
<div class="citation" id="murp12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Murp12<span class="fn-bracket">]</span></span>
<p>Murphy, K. P.: Machine learning: a probabilistic perspective. MIT press, 2012.</p>
</div>
<div class="citation" id="pave08" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PaVe08<span class="fn-bracket">]</span></span>
<p>Palomar, D. P.; Verdú, S.: Lautum information. In: IEEE transactions on information theory, Vol. 54, No. 3, 2008, P. 964-975.</p>
</div>
<div class="citation" id="stve98" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>StVe98<span class="fn-bracket">]</span></span>
<p>Studený, M.; Vejnarová, J.: The multiinformation function as a tool for measuring stochastic dependence. In: Learning in graphical models. Springer Netherlands, 1998, P. 261-297.</p>
</div>
<div class="citation" id="vewe06" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VeWe06<span class="fn-bracket">]</span></span>
<p>Verdú, S.; Weissman, T.: Erasure entropy. In: Proc. IEEE International Symposium on Information Theory, 2006, P. 98-102.</p>
</div>
<div class="citation" id="wata60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wata60<span class="fn-bracket">]</span></span>
<p>Watanabe, S.: Information theoretical analysis of multivariate correlation. In: IBM Journal of research and development, Vol. 4, No. 1, 1960, P. 66-82.</p>
</div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.divergence_jensenshannon">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">divergence_jensenshannon</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#divergence_jensenshannon"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.divergence_jensenshannon" title="Link to this definition">¶</a></dt>
<dd><p>Returns the Jensen-Shannon divergence [Lin91] between arrays X and Y, each
containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P_X\)</span>, <span class="math notranslate nohighlight">\(P_Y\)</span> respectively probability
distributions with common domain, associated with discrete random variables
<span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, the Jensen-Shannon divergence
<span class="math notranslate nohighlight">\(D_{\mathrm{JS}}(P_X \parallel P_Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{\mathrm{JS}}(P_X \parallel P_Y) =
\frac{1}{2} D_{\mathrm{KL}}(P_X \parallel M) +
\frac{1}{2} D_{\mathrm{KL}}(P_Y \parallel M)\]</div>
<p>where <span class="math notranslate nohighlight">\(M = \frac{1}{2}(P_X + P_Y)\)</span> and where
<span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(\cdot \parallel \cdot)\)</span> denotes the
Kullback-Leibler divergence.</p>
<p><strong>Estimation</strong>:</p>
<p>Jensen-Shannon divergence is estimated based on frequency tables. See below
for a list of available estimators.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[:-1]==Y.shape[:-1]. Successive realisations of a random
variable are indexed by the last axis in the respective arrays;
multiple random variables in X and Y may be specified using preceding
axes of the respective arrays (random variables are paired
<strong>one-to-one</strong> between X and Y). When X.ndim==Y.ndim==1, returns a
scalar. When X.ndim&gt;1 and Y.ndim&gt;1, returns an array of estimated
divergence values with dimensions X.shape[:-1]. Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
respective arrays; multiple random variables in X and Y may be
specified using preceding axes of the respective arrays (random
variables are paired <strong>many-to-many</strong> between X and Y). When
X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or Y.ndim&gt;1, returns
an array of estimated divergence values with dimensions
np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to divergence_jensenshannon(X, X, … ). Thus,
a shorthand syntax for computing Jensen-Shannon divergence (in bits)
between all pairs of random variables in X is
divergence_jensenshannon(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.divergence_jensenshannon_pmf">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">divergence_jensenshannon_pmf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Q</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">require_valid_pmf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#divergence_jensenshannon_pmf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.divergence_jensenshannon_pmf" title="Link to this definition">¶</a></dt>
<dd><p>Returns the Jensen-Shannon divergence [Lin91] between arrays P and Q, each
representing a discrete probability distribution.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(Q\)</span> probability distributions with common
domain, the Jensen-Shannon divergence
<span class="math notranslate nohighlight">\(D_{\mathrm{JS}}(P \parallel Q)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{\mathrm{JS}}(P \parallel Q) =
\frac{1}{2} D_{\mathrm{KL}}(P \parallel M) +
\frac{1}{2} D_{\mathrm{KL}}(Q \parallel M)\]</div>
<p>where <span class="math notranslate nohighlight">\(M = \frac{1}{2}(P + Q)\)</span> and where
<span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(\cdot \parallel \cdot)\)</span> denotes the
Kullback-Leibler divergence.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>P, Q<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape==Q.shape.
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>one-to-one</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of
divergence values with dimensions P.shape[:-1]. Neither P nor Q may
contain (floating point) NaN values.</p>
<p><em>cartesian_product==True and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape[-1]==Q.shape[-1].
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>many-to-many</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of
divergence values with dimensions np.append(P.shape[:-1],Q.shape[:-1]).
Neither P nor Q may contain (floating point) NaN values.</p>
<p><em>Q is None</em>: Equivalent to divergence_jensenshannon_pmf(P, P, … ).
Thus, a shorthand syntax for computing Jensen-Shannon divergence (in
bits) between all pairs of probability distributions in P is
divergence_jensenshannon_pmf(P).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether probability distributions are paired <strong>one-to-one</strong>
between P and Q (cartesian_product==False, the default value) or
<strong>many-to-many</strong> between P and Q (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>require_valid_pmf<span class="classifier">boolean</span></dt><dd><p>When set to True (the default value), verifies that probability mass
assignments in each distribution sum to 1. When set to False, no such
test is performed, thus allowing incomplete probability distributions
to be processed.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.divergence_kullbackleibler">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">divergence_kullbackleibler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#divergence_kullbackleibler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.divergence_kullbackleibler" title="Link to this definition">¶</a></dt>
<dd><p>Returns the Kullback-Leibler divergence (see e.g. [CoTh06]) between arrays
X and Y, each containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P_X(x)\)</span>, <span class="math notranslate nohighlight">\(P_Y(x)\)</span> respectively the probability
of observing an outcome <span class="math notranslate nohighlight">\(x\)</span> with discrete random variables <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(Y\)</span>, the Kullback-Leibler divergence
<span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(P_X\parallel P_Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{\mathrm{KL}}(P_X \parallel P_Y) =
-\sum_x {P_X(x) \log {\frac{P_Y(x)}{P_X(x)}}}.\]</div>
<p><strong>Estimation</strong>:</p>
<p>Kullback-Leibler divergence is estimated based on frequency tables, using
the following functions:</p>
<blockquote>
<div><p>entropy_cross()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
Kullback-Leibler divergence is a non-negative quantity, depending on the
chosen estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[:-1]==Y.shape[:-1]. Successive realisations of a random
variable are indexed by the last axis in the respective arrays;
multiple random variables in X and Y may be specified using preceding
axes of the respective arrays (random variables are paired
<strong>one-to-one</strong> between X and Y). When X.ndim==Y.ndim==1, returns a
scalar. When X.ndim&gt;1 and Y.ndim&gt;1, returns an array of estimated
divergence values with dimensions X.shape[:-1]. Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
respective arrays; multiple random variables in X and Y may be
specified using preceding axes of the respective arrays (random
variables are paired <strong>many-to-many</strong> between X and Y). When
X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or Y.ndim&gt;1, returns
an array of estimated divergence values with dimensions
np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to divergence_kullbackleibler(X, X, … ).
Thus, a shorthand syntax for computing Kullback-Leibler divergence (in
bits) between all pairs of random variables in X is
divergence_kullbackleibler(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.divergence_kullbackleibler_pmf">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">divergence_kullbackleibler_pmf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Q</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">require_valid_pmf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#divergence_kullbackleibler_pmf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.divergence_kullbackleibler_pmf" title="Link to this definition">¶</a></dt>
<dd><p>Returns the Kullback-Leibler divergence (see e.g. [CoTh06]) between arrays
P and Q, each representing a discrete probability distribution.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P(x)\)</span>, <span class="math notranslate nohighlight">\(Q(x)\)</span> respectively the probability mass
associated with observing an outcome <span class="math notranslate nohighlight">\(x\)</span> under distributions
<span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(Q\)</span>, the Kullback-Leibler divergence
<span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(P \parallel Q)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{\mathrm{KL}}(P \parallel Q) =
-\sum_x {P(x) \log {\frac{Q(x)}{P(x)}}}.\]</div>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>P, Q<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape==Q.shape.
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>one-to-one</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of
divergence values with dimensions P.shape[:-1]. Neither P nor Q may
contain (floating point) NaN values.</p>
<p><em>cartesian_product==True and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape[-1]==Q.shape[-1].
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>many-to-many</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of
divergence values with dimensions np.append(P.shape[:-1],Q.shape[:-1]).
Neither P nor Q may contain (floating point) NaN values.</p>
<p><em>Q is None</em>: Equivalent to divergence_kullbackleibler_pmf(P, P, … ).
Thus, a shorthand syntax for computing Kullback-Leibler divergence (in
bits) between all pairs of probability distributions in P is
divergence_kullbackleibler_pmf(P).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether probability distributions are paired <strong>one-to-one</strong>
between P and Q (cartesian_product==False, the default value) or
<strong>many-to-many</strong> between P and Q (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>require_valid_pmf<span class="classifier">boolean</span></dt><dd><p>When set to True (the default value), verifies that probability mass
assignments in each distribution sum to 1. When set to False, no such
test is performed, thus allowing incomplete probability distributions
to be processed.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.divergence_kullbackleibler_symmetrised">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">divergence_kullbackleibler_symmetrised</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#divergence_kullbackleibler_symmetrised"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.divergence_kullbackleibler_symmetrised" title="Link to this definition">¶</a></dt>
<dd><p>Returns the symmetrised Kullback-Leibler divergence [Lin91] between arrays
X and Y, each containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P_X\)</span>, <span class="math notranslate nohighlight">\(P_Y\)</span> respectively probability
distributions with common domain, associated with discrete random variables
<span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, the symmetrised Kullback-Leibler divergence
<span class="math notranslate nohighlight">\(D_{\mathrm{SKL}}(P_X \parallel P_Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{\mathrm{SKL}}(P_X \parallel P_Y) =
D_{\mathrm{KL}}(P_X \parallel P_Y) +
D_{\mathrm{KL}}(P_Y \parallel P_X)\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(\cdot \parallel \cdot)\)</span> denotes the
Kullback-Leibler divergence.</p>
<p><strong>Estimation</strong>:</p>
<p>Symmetrised Kullback-Leibler divergence is estimated based on frequency
tables, using the following functions:</p>
<blockquote>
<div><p>entropy_cross()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
symmetrised Kullback-Leibler divergence is a non-negative quantity,
depending on the chosen estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[:-1]==Y.shape[:-1]. Successive realisations of a random
variable are indexed by the last axis in the respective arrays;
multiple random variables in X and Y may be specified using preceding
axes of the respective arrays (random variables are paired
<strong>one-to-one</strong> between X and Y). When X.ndim==Y.ndim==1, returns a
scalar. When X.ndim&gt;1 and Y.ndim&gt;1, returns an array of estimated
divergence values with dimensions X.shape[:-1]. Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
respective arrays; multiple random variables in X and Y may be
specified using preceding axes of the respective arrays (random
variables are paired <strong>many-to-many</strong> between X and Y). When
X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or Y.ndim&gt;1, returns
an array of estimated divergence values with dimensions
np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to divergence_kullbackleibler_symmetrised(X, X,
… ). Thus, a shorthand syntax for computing symmetrised
Kullback-Leibler divergence (in bits) between all pairs of random
variables in X is divergence_kullbackleibler_symmetrised(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.divergence_kullbackleibler_symmetrised_pmf">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">divergence_kullbackleibler_symmetrised_pmf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Q</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">require_valid_pmf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#divergence_kullbackleibler_symmetrised_pmf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.divergence_kullbackleibler_symmetrised_pmf" title="Link to this definition">¶</a></dt>
<dd><p>Returns the symmetrised Kullback-Leibler divergence [Lin91] between arrays
P and Q, each representing a discrete probability distribution.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(Q\)</span> probability distributions with common
domain, the symmetrised Kullback-Leibler divergence
<span class="math notranslate nohighlight">\(D_{\mathrm{SKL}}(P \parallel Q)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{\mathrm{SKL}}(P \parallel Q) =
D_{\mathrm{KL}}(P \parallel Q) +
D_{\mathrm{KL}}(Q \parallel P)\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(\cdot \parallel \cdot)\)</span> denotes the
Kullback-Leibler divergence.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>P, Q<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape==Q.shape.
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>one-to-one</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of
divergence values with dimensions P.shape[:-1]. Neither P nor Q may
contain (floating point) NaN values.</p>
<p><em>cartesian_product==True and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape[-1]==Q.shape[-1].
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>many-to-many</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of
divergence values with dimensions np.append(P.shape[:-1],Q.shape[:-1]).
Neither P nor Q may contain (floating point) NaN values.</p>
<p><em>Q is None</em>: Equivalent to
divergence_kullbackleibler_symmetrised_pmf(P, P, … ). Thus, a
shorthand syntax for computing symmetrised Kullback-Leibler divergence
(in bits) between all pairs of probability distributions in P is
divergence_kullbackleibler_symmetrised_pmf(P).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether probability distributions are paired <strong>one-to-one</strong>
between P and Q (cartesian_product==False, the default value) or
<strong>many-to-many</strong> between P and Q (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>require_valid_pmf<span class="classifier">boolean</span></dt><dd><p>When set to True (the default value), verifies that probability mass
assignments in each distribution sum to 1. When set to False, no such
test is performed, thus allowing incomplete probability distributions
to be processed.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated entropy (see e.g. [CoTh06]) for an array X containing
realisations of a discrete random variable.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P(x)\)</span> the probability of observing outcome <span class="math notranslate nohighlight">\(x\)</span>
of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, the entropy <span class="math notranslate nohighlight">\(H(X)\)</span> is
defined as:</p>
<div class="math notranslate nohighlight">
\[H(X) = -\sum_x {P(x) \log {P(x)}}.\]</div>
<p><strong>Estimation</strong>:</p>
<p>Entropy is estimated based on frequency tables. See below for a list of
available estimators.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns a scalar. When X.ndim&gt;1, returns an array of
estimated entropies with dimensions X.shape[:-1].  X may not contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy_conditional">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy_conditional</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy_conditional"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy_conditional" title="Link to this definition">¶</a></dt>
<dd><p>Returns the conditional entropy (see e.g. [CoTh06]) between arrays X and Y,
each containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, the conditional
entropy <span class="math notranslate nohighlight">\(H(X|Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[H(X|Y) = H(X,Y) - H(Y)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot,\cdot)\)</span> denotes the joint entropy and where
<span class="math notranslate nohighlight">\(H(\cdot)\)</span> denotes the entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Conditional entropy is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
conditional entropy is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape==Y.shape. Successive realisations of a random variable are
indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>one-to-one</strong> between X
and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 and
Y.ndim&gt;1, returns an array of estimated conditional entropies with
dimensions X.shape[:-1]. Neither X nor Y may contain (floating point)
NaN values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[-1]==Y.shape[-1]. Successive realisations of a random variable
are indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>many-to-many</strong> between
X and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or
Y.ndim&gt;1, returns an array of estimated conditional entropies with
dimensions np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to entropy_conditional(X, X, … ). Thus, a
shorthand syntax for computing conditional entropies (in bits) between
all pairs of random variables in X is entropy_conditional(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
<p>NB: When specifying alphabets, an alphabet of possible joint outcomes
is always implicit from the alphabets of possible (marginal) outcomes
in Alphabet_X, Alphabet_Y. For example, specifying
Alphabet_X=Alphabet_Y=np.array(((1,2)) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy_cross">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy_cross</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy_cross"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy_cross" title="Link to this definition">¶</a></dt>
<dd><p>Returns the cross entropy (see e.g. [Murp12]) between arrays X and Y, each
containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P_X(x)\)</span>, <span class="math notranslate nohighlight">\(P_Y(x)\)</span> respectively the probability
of observing an outcome <span class="math notranslate nohighlight">\(x\)</span> with discrete random variables <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(Y\)</span>, the cross entropy <span class="math notranslate nohighlight">\(H^\times(X,Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[H^\times(X,Y) = -\sum_x {P_X(x) \log {P_Y(x)}}.\]</div>
<p><strong>Estimation</strong>:</p>
<p>Cross entropy is estimated based on frequency tables. See below for a list
of available estimators.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[:-1]==Y.shape[:-1]. Successive realisations of a random
variable are indexed by the last axis in the respective arrays;
multiple random variables in X and Y may be specified using preceding
axes of the respective arrays (random variables are paired
<strong>one-to-one</strong> between X and Y). When X.ndim==Y.ndim==1, returns a
scalar. When X.ndim&gt;1 and Y.ndim&gt;1, returns an array of estimated cross
entropies with dimensions X.shape[:-1]. Neither X nor Y may contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
respective arrays; multiple random variables in X and Y may be
specified using preceding axes of the respective arrays (random
variables are paired <strong>many-to-many</strong> between X and Y). When
X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or Y.ndim&gt;1, returns
an array of estimated cross entropies with dimensions
np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to entropy_cross(X, X, … ). Thus, a shorthand
syntax for computing cross entropies (in bits) between all pairs of
random variables in X is entropy_cross(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy_cross_pmf">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy_cross_pmf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Q</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">require_valid_pmf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy_cross_pmf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy_cross_pmf" title="Link to this definition">¶</a></dt>
<dd><p>Returns the cross entropy (see e.g. [Murp12]) between arrays P and Q, each
representing a discrete probability distribution.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P(x)\)</span>, <span class="math notranslate nohighlight">\(Q(x)\)</span> respectively the probability mass
associated with observing an outcome <span class="math notranslate nohighlight">\(x\)</span> under distributions
<span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(Q\)</span>, the cross entropy <span class="math notranslate nohighlight">\(H^\times(P,Q)\)</span> is defined
as:</p>
<div class="math notranslate nohighlight">
\[H^\times(P,Q) = -\sum_x {P(x) \log {Q(x)}}.\]</div>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>P, Q<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape==Q.shape.
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>one-to-one</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of cross
entropies with dimensions P.shape[:-1]. Neither P nor Q may contain
(floating point) NaN values.</p>
<p><em>cartesian_product==True and Q is not None</em>: P and Q are arrays
containing probability mass assignments, with P.shape[-1]==Q.shape[-1].
Probabilities in a distribution are indexed by the last axis in the
respective arrays; multiple probability distributions in P and Q may be
specified using preceding axes of the respective arrays (distributions
are paired <strong>many-to-many</strong> between P and Q). When P.ndim==Q.ndim==1,
returns a scalar. When P.ndim&gt;1 and Q.ndim&gt;1, returns an array of cross
entropies with dimensions np.append(P.shape[:-1],Q.shape[:-1]). Neither
P nor Q may contain (floating point) NaN values.</p>
<p><em>Q is None</em>: Equivalent to entropy_cross_pmf(P, P, … ). Thus, a
shorthand syntax for computing cross entropies (in bits) between all
pairs of probability distributions in P is entropy_cross_pmf(P).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether probability distributions are paired <strong>one-to-one</strong>
between P and Q (cartesian_product==False, the default value) or
<strong>many-to-many</strong> between P and Q (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>require_valid_pmf<span class="classifier">boolean</span></dt><dd><p>When set to True (the default value), verifies that probability mass
assignments in each distribution sum to 1. When set to False, no such
test is performed, thus allowing incomplete probability distributions
to be processed.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy_joint">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy_joint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy_joint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy_joint" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated joint entropy (see e.g. [CoTh06]) for an array X
containing realisations of discrete random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P(x_1, \ldots, x_n)\)</span> the probability of jointly
observing outcomes <span class="math notranslate nohighlight">\((x_1, \ldots, x_n)\)</span> of <span class="math notranslate nohighlight">\(n\)</span> discrete random
variables <span class="math notranslate nohighlight">\((X_1, \ldots, X_n)\)</span>, the joint entropy
<span class="math notranslate nohighlight">\(H(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[H(X_1, \ldots, X_n) = -\sum_{x_1} \ldots \sum_{x_n}
{P(x_1, \ldots, x_n ) \log {P(x_1, \ldots, x_n)}}.\]</div>
<p><strong>Estimation</strong>:</p>
<p>Joint entropy is estimated based on frequency tables. See below for a list
of available estimators.</p>
<p><em>Parameters*</em>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns a scalar and is equivalent to entropy(). When
X.ndim&gt;1, returns a scalar based on jointly considering all random
variables indexed in the array. X may not contain (floating point) NaN
values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy_pmf">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy_pmf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">require_valid_pmf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy_pmf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy_pmf" title="Link to this definition">¶</a></dt>
<dd><p>Returns the entropy (see e.g. [CoTh06]) of an array P representing a
discrete probability distribution.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P(x)\)</span> the probability mass associated with observing
an outcome <span class="math notranslate nohighlight">\(x\)</span> under distribution <span class="math notranslate nohighlight">\(P\)</span>, the entropy <span class="math notranslate nohighlight">\(H(P)\)</span>
is defined as:</p>
<div class="math notranslate nohighlight">
\[H(P) = -\sum_x {P(x) \log {P(x)}}.\]</div>
<p><strong>Parameters</strong>:</p>
<dl class="simple">
<dt>P<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing probability mass assignments. Probabilities in a
distribution are indexed by the last axis in the array; multiple
probability distributions may be specified using preceding axes. When
P.ndim==1, returns a scalar. When P.ndim&gt;1, returns an array of
entropies with dimensions P.shape[:-1]. P may not contain (floating
point) NaN values.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>require_valid_pmf<span class="classifier">boolean</span></dt><dd><p>When set to True (the default value), verifies that probability mass
assignments in each distribution sum to 1. When set to False, no such
test is performed, thus allowing incomplete probability distributions
to be processed.</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.entropy_residual">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">entropy_residual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#entropy_residual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.entropy_residual" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated residual entropy [JaEC11] (also known as erasure
entropy [VeWe06]) for an array X containing realisations of discrete random
variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the residual
entropy <span class="math notranslate nohighlight">\(R(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[R(X_1, \ldots, X_n) = H(X_1, \ldots, X_n) - B(X_1, \ldots, X_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot, \ldots, \cdot)\)</span> denotes the joint entropy and
where <span class="math notranslate nohighlight">\(B(\cdot, \ldots, \cdot)\)</span> denotes the binding information.</p>
<p><strong>Estimation</strong>:</p>
<p>Residual information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although residual
information is a non-negative quantity, depending on the chosen estimator
the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns a scalar and is equivalent to entropy(). When
X.ndim&gt;1, returns a scalar based on jointly considering all random
variables indexed in the array. X may not contain (floating point) NaN
values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_binding">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_binding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_binding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_binding" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated binding information [AbPl12] (also known as dual
total correlation [Han78]) for an array X containing realisations of
discrete random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the binding
information <span class="math notranslate nohighlight">\(B(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[B(X_1, \ldots, X_n) = H(X_1, \ldots, X_n) -
\sum_{i=1}^{n} H(X_i | X_1, \ldots X_{i-1}, X_{i+1}, \ldots, X_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot)\)</span> denotes the entropy and where
<span class="math notranslate nohighlight">\(H(\cdot | \cdot)\)</span> denotes the conditional entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Binding information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although binding
information is a non-negative quantity, depending on the chosen estimator
the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns the scalar 0. When X.ndim&gt;1, returns a scalar
based on jointly considering all random variables indexed in the array.
X may not contain (floating point) NaN values. Missing data may be
specified using numpy masked arrays, as well as using standard
numpy array/array-like objects; see below for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_co">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_co</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_co"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_co" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated co-information [Bell03] for an array X containing
realisations of discrete random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the
co-information <span class="math notranslate nohighlight">\(I(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[I(X_1, \ldots, X_n) = - \sum_{T \subseteq \{1,\ldots, n\}}
(-1)^{|T|}  H(X_i : i \in T)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(X_i : i \in T)\)</span> denotes the joint entropy of the subset of
random variables specified by <span class="math notranslate nohighlight">\(T\)</span>. Thus, co-information is an
alternating sum of joint entropies, with the sets of random variables used
to compute the joint entropy in each term selected from the power set of
available random variables.</p>
<p>Note that co-information is equal in magnitude to the interaction
information <span class="math notranslate nohighlight">\(\mathrm{Int}(X_1, \ldots, X_n)\)</span>, with equality for the
case where <span class="math notranslate nohighlight">\(n\)</span> is even,</p>
<div class="math notranslate nohighlight">
\[I(X_1, \ldots, X_n) = (-1)^n \mathrm{Int}(X_1, \ldots, X_n).\]</div>
<p><strong>Estimation</strong>:</p>
<p>Co-information is estimated based on frequency tables, using the following
functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
co-information is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns a scalar and is equivalent to entropy(). When
X.ndim&gt;1, returns a scalar based on jointly considering all random
variables indexed in the array. X may not contain (floating point) NaN
values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_enigmatic">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_enigmatic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_enigmatic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_enigmatic" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated enigmatic information [JaEC11] for an array X
containing realisations of discrete random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the enigmatic
information <span class="math notranslate nohighlight">\(Q(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[Q(X_1, \ldots, X_n) = T(X_1, \ldots, X_n) - B(X_1, \ldots, X_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(T(\cdot, \ldots, \cdot)\)</span> denotes the multi-information and
where <span class="math notranslate nohighlight">\(B(\cdot, \ldots, \cdot)\)</span> denotes the binding information.</p>
<p><strong>Estimation</strong>:</p>
<p>Enigmatic information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although enigmatic
information is a non-negative quantity, depending on the chosen estimator
the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns the scalar 0. When X.ndim&gt;1, returns a scalar
based on jointly considering all random variables indexed in the array.
X may not contain (floating point) NaN values. Missing data may be
specified using numpy masked arrays, as well as using standard
numpy array/array-like objects; see below for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_exogenous_local">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_exogenous_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_exogenous_local"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_exogenous_local" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated exogenous local information [JaEC11] for an array X
containing realisations of discrete random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the exogenous
local information <span class="math notranslate nohighlight">\(W(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[W(X_1, \ldots, X_n) = T(X_1, \ldots, X_n) + B(X_1, \ldots, X_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(T(\cdot, \ldots, \cdot)\)</span> denotes the multi-information and
where <span class="math notranslate nohighlight">\(B(\cdot, \ldots, \cdot)\)</span> denotes the binding information.</p>
<p><strong>Estimation</strong>:</p>
<p>Exogenous local information is estimated based on frequency tables, using
the following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although exogenous
local information is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns the scalar 0. When X.ndim&gt;1, returns a scalar
based on jointly considering all random variables indexed in the array.
X may not contain (floating point) NaN values. Missing data may be
specified using numpy masked arrays, as well as using standard
numpy array/array-like objects; see below for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_interaction">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_interaction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_interaction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_interaction" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated interaction information [JaBr03] for an array X
containing realisations of discrete random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the interaction
information <span class="math notranslate nohighlight">\(\mathrm{Int}(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\mathrm{Int}(X_1, \ldots, X_n) = - \sum_{T \subseteq
\{1,\ldots, n\}} (-1)^{n-|T|}  H(X_i : i \in T)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(X_i : i \in T)\)</span> denotes the joint entropy of the subset of
random variables specified by <span class="math notranslate nohighlight">\(T\)</span>. Thus, interaction information is
an alternating sum of joint entropies, with the sets of random variables
used to compute the joint entropy in each term selected from the power set
of available random variables.</p>
<p>Note that interaction information is equal in magnitude to the
co-information <span class="math notranslate nohighlight">\(I(X_1, \ldots, X_n)\)</span>, with equality for the case
where <span class="math notranslate nohighlight">\(n\)</span> is even,</p>
<div class="math notranslate nohighlight">
\[\mathrm{Int}(X_1, \ldots, X_n) = (-1)^n I(X_1, \ldots, X_n).\]</div>
<p><strong>Estimation</strong>:</p>
<p>Interaction information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
interaction information is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns a scalar and is equivalent to -1*entropy().
When X.ndim&gt;1, returns a scalar based on jointly considering all random
variables indexed in the array. X may not contain (floating point) NaN
values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_lautum">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_lautum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_lautum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_lautum" title="Link to this definition">¶</a></dt>
<dd><p>Returns the lautum information [PaVe08] between arrays X and Y, each
containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Denoting with <span class="math notranslate nohighlight">\(P_X(x)\)</span>, <span class="math notranslate nohighlight">\(P_Y(x)\)</span> respectively the probability
of observing an outcome <span class="math notranslate nohighlight">\(x\)</span> with discrete random variables <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(Y\)</span>, and denoting with <span class="math notranslate nohighlight">\(P_{XY}(x,y)\)</span> the probability of jointly
observing outcomes <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span> respectively with <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(Y\)</span>, the lautum information <span class="math notranslate nohighlight">\(L(X;Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
    L(X;Y) &amp;=&amp; -\sum_x \sum_y
    {P_X(x) P_Y(y) \log {\frac{P_X(x) P_Y(y)}{P_{XY}(x,y)}}} \\
    &amp;=&amp; D_{\mathrm{KL}}(P_X P_Y \parallel P_{XY})
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{\mathrm{KL}}(\cdot \parallel \cdot)\)</span> denotes the
Kullback-Leibler divergence. Note that <em>lautum</em> is <em>mutual</em> spelt
backwards; denoting with <span class="math notranslate nohighlight">\(I(\cdot;\cdot)\)</span> the mutual information it
may be shown (see e.g. [CoTh06]) that</p>
<div class="math notranslate nohighlight">
\[\begin{eqnarray}
    I(X;Y) &amp;=&amp; D_{\mathrm{KL}}(P_{XY} \parallel P_X P_Y).
\end{eqnarray}\]</div>
<p><strong>Estimation</strong>:</p>
<p>Lautum information is estimated based on frequency tables. See below for a
list of available estimators.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[:-1]==Y.shape[:-1]. Successive realisations of a random
variable are indexed by the last axis in the respective arrays;
multiple random variables in X and Y may be specified using preceding
axes of the respective arrays (random variables are paired
<strong>one-to-one</strong> between X and Y). When X.ndim==Y.ndim==1, returns a
scalar. When X.ndim&gt;1 and Y.ndim&gt;1, returns an array of estimated
information values with dimensions X.shape[:-1]. Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
respective arrays; multiple random variables in X and Y may be
specified using preceding axes of the respective arrays (random
variables are paired <strong>many-to-many</strong> between X and Y). When
X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or Y.ndim&gt;1, returns
an array of estimated information values with dimensions
np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may contain
(floating point) NaN values. Missing data may be specified using numpy
masked arrays, as well as using standard numpy array/array-like
objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to information_lautum(X, X, … ). Thus, a
shorthand syntax for computing lautum information (in bits) between all
pairs of random variables in X is information_lautum(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
<p>NB: When specifying alphabets, an alphabet of possible joint outcomes
is always implicit from the alphabets of possible (marginal) outcomes
in Alphabet_X, Alphabet_Y. For example, specifying
Alphabet_X=Alphabet_Y=np.array(((1,2)) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_multi">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_multi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_multi"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_multi" title="Link to this definition">¶</a></dt>
<dd><p>Returns the estimated multi-information [StVe98] (also known as total
correlation [Wata60]) for an array X containing realisations of discrete
random variables.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the
multi-information <span class="math notranslate nohighlight">\(T(X_1, \ldots, X_n)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[T(X_1, \ldots, X_n) = \left( \sum_{i=1}^{n} H(X_i) \right) -
H(X_1, \ldots, X_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot)\)</span> denotes the entropy and where
<span class="math notranslate nohighlight">\(H(\cdot, \ldots, \cdot)\)</span> denotes the joint entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Multi-information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
multi-information is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p>An array containing discrete random variable realisations. Successive
realisations of a random variable are indexed by the last axis in the
array; multiple random variables may be specified using preceding axes.
When X.ndim==1, returns the scalar 0. When X.ndim&gt;1, returns a scalar
based on jointly considering all random variables indexed in the array.
X may not contain (floating point) NaN values. Missing data may be
specified using numpy masked arrays, as well as using standard
numpy array/array-like objects; see below for details.</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X<span class="classifier">numpy array (or array-like object such as a list of     immutables, as accepted by np.array())</span></dt><dd><p>An array specifying the alphabet/alphabets of possible outcomes that
random variable realisations in array X may assume. Defaults to None,
in which case the alphabet/alphabets of possible outcomes is/are
implicitly based the observed outcomes in array X, with no additional,
unobserved outcomes. In combination with any estimator other than
maximum likelihood, it may be useful to specify alphabets including
unobserved outcomes. For such cases, successive possible outcomes of a
random variable are indexed by the last axis in Alphabet_X; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1]. Alphabets of different sizes may
be specified either using numpy masked arrays, or by padding with the
chosen placeholder fill_value.</p>
<p>NB: When specifying multiple alphabets, an alphabet of possible joint
outcomes is always implicit from the alphabets of possible (marginal)
outcomes in Alphabet_X. For example, specifying
Alphabet_X=np.array(((1,2),(1,2))) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True, an additional dimension of length one is appended to
the returned array, facilitating any broadcast operations required by
the user (defaults to False).</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_mutual">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_mutual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_mutual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_mutual" title="Link to this definition">¶</a></dt>
<dd><p>Returns the mutual information (see e.g. [CoTh06]) between arrays X and Y,
each containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, the mutual
information <span class="math notranslate nohighlight">\(I(X;Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) = H(X) - H(X|Y)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot)\)</span> denotes the entropy and where
<span class="math notranslate nohighlight">\(H(\cdot|\cdot)\)</span> denotes the conditional entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Mutual information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although mutual
information is a non-negative quantity, depending on the chosen estimator
the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape==Y.shape. Successive realisations of a random variable are
indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>one-to-one</strong> between X
and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 and
Y.ndim&gt;1, returns an array of estimated mutual information values with
dimensions X.shape[:-1]. Neither X nor Y may contain (floating point)
NaN values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[-1]==Y.shape[-1]. Successive realisations of a random variable
are indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>many-to-many</strong> between
X and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or
Y.ndim&gt;1, returns an array of estimated mutual information values with
dimensions np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to information_mutual(X, X, … ). Thus, a
shorthand syntax for computing mutual information (in bits) between all
pairs of random variables in X is information_mutual(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
<p>NB: When specifying alphabets, an alphabet of possible joint outcomes
is always implicit from the alphabets of possible (marginal) outcomes
in Alphabet_X, Alphabet_Y. For example, specifying
Alphabet_X=Alphabet_Y=np.array(((1,2)) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_mutual_conditional">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_mutual_conditional</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Z</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_mutual_conditional"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_mutual_conditional" title="Link to this definition">¶</a></dt>
<dd><p>Returns the conditional mutual information (see e.g. [CoTh06]) between
arrays X and Y given array Z, each containing discrete random variable
realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>,  <span class="math notranslate nohighlight">\(Z\)</span>, the
conditional mutual information <span class="math notranslate nohighlight">\(I(X;Y|Z)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[I(X;Y|Z) = H(X|Z) - H(X|Y,Z)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot|\cdot)\)</span> denotes the conditional entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Conditional mutual information is estimated based on frequency tables,
using the following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although
conditional mutual information is a non-negative quantity, depending on the
chosen estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y,Z<span class="classifier">numpy array (or array-like object such as a list of immutables,     as accepted by np.array())</span></dt><dd><p><em>cartesian_product==False</em>: X,Y,Z are arrays containing discrete random
variable realisations, with X.shape==Y.shape==Z.shape. Successive
realisations of a random variable are indexed by the last axis in the
respective arrays; multiple random variables in X,Y,Z may be specified
using preceding axes of the respective arrays (random variables are
paired <strong>one-to-one</strong> between X,Y,Z). When X.ndim==Y.ndim==Z.ndim==1,
returns a scalar. When X.ndim&gt;1 and Y.ndim&gt;1 and Z.ndim&gt;1, returns an
array of estimated conditional mutual information values with
dimensions X.shape[:-1]. Neither X nor Y nor Z may contain (floating
point) NaN values. Missing data may be specified using numpy masked
arrays, as well as using standard numpy array/array-like objects;
see below for details.</p>
<p><em>cartesian_product==True</em>: X,Y,Z are arrays containing discrete random
variable realisations, with X.shape[-1]==Y.shape[-1]==Z.shape[-1].
Successive realisations of a random variable are indexed by the last
axis in the respective arrays; multiple random variables in X,Y,Z may
be specified using preceding axes of the respective arrays (random
variables are paired <strong>many-to-many</strong> between X,Y,Z). When
X.ndim==Y.ndim==Z.ndim==1, returns a scalar. When X.ndim&gt;1 or Y.ndim&gt;1
or Z.ndim&gt;1, returns an array of estimated conditional mutual
information values with dimensions
np.append(X.shape[:-1],Y.shape[:-1],Z.shape[:-1]). Neither X nor Y nor
Z may contain (floating point) NaN values. Missing data may be
specified using numpy masked arrays, as well as using standard
numpy array/array-like objects; see below for details.</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between
X,Y,Z (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X,Y,Z (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y, Alphabet_Z<span class="classifier">numpy array (or array-like object     such as a list of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y, Z may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y, Z
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y, Alphabet_Z respectively; multiple
alphabets may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y and Z).
Alphabets of different sizes may be specified either using numpy masked
arrays, or by padding with the chosen placeholder fill_value.</p>
<p>NB: When specifying alphabets, an alphabet of possible joint outcomes
is always implicit from the alphabets of possible (marginal) outcomes
in Alphabet_X, Alphabet_Y, Alphabet_Z. For example, specifying
Alphabet_X=Alphabet_Y=Alphabet_Z=np.array(((1,2)) implies an alphabet
of possible joint outcomes
np.array((1,1,1,1,2,2,2,2),((1,1,2,2,1,1,2,2),(1,2,1,2,1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_mutual_normalised">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_mutual_normalised</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Y'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_mutual_normalised"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_mutual_normalised" title="Link to this definition">¶</a></dt>
<dd><p>Returns the normalised mutual information between arrays X and Y, each
containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, the normalised mutual
information <span class="math notranslate nohighlight">\(NI(X;Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[NI(X;Y) = \frac{I(X;Y)}{C_n}\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> denotes the mutual information and where <span class="math notranslate nohighlight">\(C_n\)</span>
denotes a normalisation factor. Normalised mutual information is a
dimensionless quantity, with <span class="math notranslate nohighlight">\(C_n\)</span> alternatively defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
  C_{\text{X}} &amp;=&amp; H(X) \\
  C_{\text{Y}} &amp;=&amp; H(Y) \\
  C_{\text{X+Y}} &amp;=&amp; H(X) + H(Y) \\
  C_{\text{MIN}} &amp;=&amp; \min \{ H(X), H(Y) \} \\
  C_{\text{MAX}} &amp;=&amp; \max \{ H(X), H(Y) \} \\
  C_{\text{XY}} &amp;=&amp; H(X,Y) \\
  C_{\text{SQRT}} &amp;=&amp; \sqrt{H(X) H(Y)}
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot)\)</span> and <span class="math notranslate nohighlight">\(H(\cdot,\cdot)\)</span> respectively denote
the entropy and joint entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Normalised mutual information is estimated based on frequency tables, using
the following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although normalised
mutual information is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape==Y.shape. Successive realisations of a random variable are
indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>one-to-one</strong> between X
and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 and
Y.ndim&gt;1, returns an array of estimated normalised information values
with dimensions X.shape[:-1]. Neither X nor Y may contain (floating
point) NaN values. Missing data may be specified using numpy masked
arrays, as well as using standard numpy array/array-like objects;
see below for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[-1]==Y.shape[-1]. Successive realisations of a random variable
are indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>many-to-many</strong> between
X and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or
Y.ndim&gt;1, returns an array of estimated normalised information values
with dimensions np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y
may contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to information_mutual_normalised(X, X,
norm_factor, True). Thus, a shorthand syntax for computing normalised
mutual information (based on C_n = C_Y as defined above) between all
pairs of random variables in X is information_mutual_normalised(X).</p>
</dd>
<dt>norm_factor<span class="classifier">string</span></dt><dd><p>The desired normalisation factor, specified as a string. Internally,
the supplied string is converted to upper case and spaces are
discarded. Subsequently, the function tests for one of the following
string values, each corresponding to an alternative normalisation
factor as defined above:</p>
<p><em>‘X’</em></p>
<p><em>‘Y’ (the default value)</em></p>
<p><em>‘X+Y’ (equivalently ‘Y+X’)</em></p>
<p><em>‘MIN’</em></p>
<p><em>‘MAX’</em></p>
<p><em>‘XY’ (equivalently YX)</em></p>
<p><em>‘SQRT’</em></p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
<p>NB: When specifying alphabets, an alphabet of possible joint outcomes
is always implicit from the alphabets of possible (marginal) outcomes
in Alphabet_X, Alphabet_Y. For example, specifying
Alphabet_X=Alphabet_Y=np.array(((1,2)) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="discrete_random_variable.information_variation">
<span class="sig-prename descclassname"><span class="pre">discrete_random_variable.</span></span><span class="sig-name descname"><span class="pre">information_variation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cartesian_product</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ML'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Alphabet_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/discrete_random_variable.html#information_variation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#discrete_random_variable.information_variation" title="Link to this definition">¶</a></dt>
<dd><p>Returns the variation of information [Meil03] between arrays X and Y, each
containing discrete random variable realisations.</p>
<p><strong>Mathematical definition</strong>:</p>
<p>Given discrete random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, the variation of
information <span class="math notranslate nohighlight">\(VI(X;Y)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[VI(X;Y) = H(X|Y) + H(Y|X)\]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cdot|\cdot)\)</span> denotes the conditional entropy.</p>
<p><strong>Estimation</strong>:</p>
<p>Variation of information is estimated based on frequency tables, using the
following functions:</p>
<blockquote>
<div><p>entropy_joint()</p>
<p>entropy()</p>
</div></blockquote>
<p>See below for a list of available estimators. Note that although variation
of information is a non-negative quantity, depending on the chosen
estimator the obtained estimate may be negative.</p>
<p><strong>Parameters</strong>:</p>
<dl>
<dt>X,Y<span class="classifier">numpy array (or array-like object such as a list of immutables, as     accepted by np.array())</span></dt><dd><p><em>cartesian_product==False and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape==Y.shape. Successive realisations of a random variable are
indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>one-to-one</strong> between X
and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 and
Y.ndim&gt;1, returns an array of estimated information values with
dimensions X.shape[:-1]. Neither X nor Y may contain (floating point)
NaN values. Missing data may be specified using numpy masked arrays, as
well as using standard numpy array/array-like objects; see below
for details.</p>
<p><em>cartesian_product==True and Y is not None</em>: X and Y are arrays
containing discrete random variable realisations, with
X.shape[-1]==Y.shape[-1]. Successive realisations of a random variable
are indexed by the last axis in the respective arrays; multiple random
variables in X and Y may be specified using preceding axes of the
respective arrays (random variables are paired <strong>many-to-many</strong> between
X and Y). When X.ndim==Y.ndim==1, returns a scalar. When X.ndim&gt;1 or
Y.ndim&gt;1, returns an array of estimated information values with
dimensions np.append(X.shape[:-1],Y.shape[:-1]). Neither X nor Y may
contain (floating point) NaN values. Missing data may be specified
using numpy masked arrays, as well as using standard numpy
array/array-like objects; see below for details.</p>
<p><em>Y is None</em>: Equivalent to information_variation(X, X, … ). Thus, a
shorthand syntax for computing variation of information (in bits)
between all pairs of random variables in X is information_variation(X).</p>
</dd>
<dt>cartesian_product<span class="classifier">boolean</span></dt><dd><p>Indicates whether random variables are paired <strong>one-to-one</strong> between X
and Y (cartesian_product==False, the default value) or <strong>many-to-many</strong>
between X and Y (cartesian_product==True).</p>
</dd>
<dt>base<span class="classifier">float</span></dt><dd><p>The desired logarithmic base (default 2).</p>
</dd>
<dt>fill_value<span class="classifier">object</span></dt><dd><p>It is possible to specify missing data using numpy masked arrays,
pandas Series/DataFrames, as well as using standard numpy
array/array-like objects with assigned placeholder values. When using
numpy masked arrays, this function invokes np.ma.filled() internally,
so that missing data are represented with the array’s object-internal
placeholder value fill_value (this function’s fill_value parameter is
ignored in such cases). When using pandas Series/DataFrames, an initial
conversion to a numpy masked array is performed. When using standard
numpy array/array-like objects, this function’s fill_value parameter is
used to specify the placeholder value for missing data (defaults to
-1).</p>
<p>Data equal to the placeholder value are subsequently ignored.</p>
</dd>
<dt>estimator<span class="classifier">str or float</span></dt><dd><p>The desired estimator (see above for details on estimators). Possible
values are:</p>
<blockquote>
<div><p><em>‘ML’ (the default value)</em> : Maximum likelihood estimator.</p>
<p><em>any floating point value</em> : Maximum a posteriori esimator using
Dirichlet prior (equivalent to maximum likelihood with pseudo-count
for each outcome as specified).</p>
<p><em>PERKS</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to 1/L, where L is the number of possible outcomes.</p>
<p><em>MINIMAX</em> : Maximum a posteriori esimator using Dirichlet prior
(equivalent to maximum likelihood with pseudo-count for each
outcome set to sqrt(N)/L, where N is the total number of
realisations and where L is the number of possible outcomes.</p>
<p><em>JAMES-STEIN</em> : James-Stein estimator [HaSt09].</p>
<p><em>GOOD-TURING</em> : Good-Turing estimator [GaSa95].</p>
</div></blockquote>
</dd>
<dt>Alphabet_X, Alphabet_Y<span class="classifier">numpy array (or array-like object such as a list     of immutables, as accepted by np.array())</span></dt><dd><p>Respectively an array specifying the alphabet/alphabets of possible
outcomes that random variable realisations in array X, Y may assume.
Defaults to None, in which case the alphabet/alphabets of possible
outcomes is/are implicitly based the observed outcomes in array X, Y
respectively, with no additional, unobserved outcomes. In combination
with any estimator other than maximum likelihood, it may be useful to
specify alphabets including unobserved outcomes. For such cases,
successive possible outcomes of a random variable are indexed by the
last axis in Alphabet_X, Alphabet_Y respectively; multiple alphabets
may be specified using preceding axes, with the requirement
X.shape[:-1]==Alphabet_X.shape[:-1] (analogously for Y). Alphabets of
different sizes may be specified either using numpy masked arrays, or
by padding with the chosen placeholder fill_value.</p>
<p>NB: When specifying alphabets, an alphabet of possible joint outcomes
is always implicit from the alphabets of possible (marginal) outcomes
in Alphabet_X, Alphabet_Y. For example, specifying
Alphabet_X=Alphabet_Y=np.array(((1,2)) implies an alphabet of possible
joint outcomes np.array(((1,1,2,2),(1,2,1,2))).</p>
</dd>
<dt>keep_dims<span class="classifier">boolean</span></dt><dd><p>When set to True and cartesian_product==False an additional dimension
of length one is appended to the returned array, facilitating any
broadcast operations required by the user (defaults to False). Has no
effect when cartesian_product==True.</p>
</dd>
</dl>
<p><strong>Implementation notes</strong>:</p>
<p>Before estimation, outcomes are mapped to the set of non-negative integers
internally, with the value -1 representing missing data. To avoid this
internal conversion step, supply integer data and use the default fill
value -1.</p>
</dd></dl>

</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="#">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">pyitlib</a><ul>
<li><a class="reference internal" href="#installation-and-codebase">Installation and codebase</a></li>
<li><a class="reference internal" href="#notes-for-getting-started">Notes for getting started</a></li>
<li><a class="reference internal" href="#module-discrete_random_variable">discrete_random_variable</a><ul>
<li><a class="reference internal" href="#discrete_random_variable.divergence_jensenshannon"><code class="docutils literal notranslate"><span class="pre">divergence_jensenshannon()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.divergence_jensenshannon_pmf"><code class="docutils literal notranslate"><span class="pre">divergence_jensenshannon_pmf()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler"><code class="docutils literal notranslate"><span class="pre">divergence_kullbackleibler()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler_pmf"><code class="docutils literal notranslate"><span class="pre">divergence_kullbackleibler_pmf()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler_symmetrised"><code class="docutils literal notranslate"><span class="pre">divergence_kullbackleibler_symmetrised()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.divergence_kullbackleibler_symmetrised_pmf"><code class="docutils literal notranslate"><span class="pre">divergence_kullbackleibler_symmetrised_pmf()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy"><code class="docutils literal notranslate"><span class="pre">entropy()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy_conditional"><code class="docutils literal notranslate"><span class="pre">entropy_conditional()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy_cross"><code class="docutils literal notranslate"><span class="pre">entropy_cross()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy_cross_pmf"><code class="docutils literal notranslate"><span class="pre">entropy_cross_pmf()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy_joint"><code class="docutils literal notranslate"><span class="pre">entropy_joint()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy_pmf"><code class="docutils literal notranslate"><span class="pre">entropy_pmf()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.entropy_residual"><code class="docutils literal notranslate"><span class="pre">entropy_residual()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_binding"><code class="docutils literal notranslate"><span class="pre">information_binding()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_co"><code class="docutils literal notranslate"><span class="pre">information_co()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_enigmatic"><code class="docutils literal notranslate"><span class="pre">information_enigmatic()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_exogenous_local"><code class="docutils literal notranslate"><span class="pre">information_exogenous_local()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_interaction"><code class="docutils literal notranslate"><span class="pre">information_interaction()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_lautum"><code class="docutils literal notranslate"><span class="pre">information_lautum()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_multi"><code class="docutils literal notranslate"><span class="pre">information_multi()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_mutual"><code class="docutils literal notranslate"><span class="pre">information_mutual()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_mutual_conditional"><code class="docutils literal notranslate"><span class="pre">information_mutual_conditional()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_mutual_normalised"><code class="docutils literal notranslate"><span class="pre">information_mutual_normalised()</span></code></a></li>
<li><a class="reference internal" href="#discrete_random_variable.information_variation"><code class="docutils literal notranslate"><span class="pre">information_variation()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">pyitlib 0.3.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">pyitlib</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2016, Peter Foster.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    </div>
  </body>
</html>